{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/ESPN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ Multi-League Extraction ---\n",
      "‚úÖ 18 teams captured for fra.1\n",
      "‚úÖ 18 teams captured for fra.2\n"
     ]
    }
   ],
   "source": [
    "# %run ./Competition_mapping.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Starting historical scrape for 36 teams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f2e592e0af46a18a325e9b34a469be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully collected 792 historical matches.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"}\n",
    "Hist = []\n",
    "\n",
    "print(f\"üìà Starting historical scrape for {len(Teams)} teams...\")\n",
    "\n",
    "# --- 2. LOOP THROUGH TEAMS ---\n",
    "for index, row in tqdm(Teams.iterrows(), total=len(Teams)):\n",
    "    # Construct the results URL for the current season\n",
    "    # Format: https://www.espn.co.uk/football/team/results/_/id/XXXX/season/2025\n",
    "    base_url = \"https://www.espn.co.uk/\"\n",
    "    # url = urljoin(base_url, row[\"LinkTeam\"]) + \"league/\" + rljoin(base_url, row[\"Competition\"])\n",
    "    url = urljoin(base_url, row[\"LinkTeam\"]) + f\"league/{row['Competition']}/season/2025\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200: continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # ESPN groups results by month/year in 'Table__Title'\n",
    "        # We need these to attach the correct year to the \"Sat, 10 Aug\" date strings\n",
    "        titles = [t.text.split(\",\")[1].strip() if \",\" in t.text else \"2025\" for t in soup.find_all(class_='Table__Title')]\n",
    "        \n",
    "        # Find all tables of results\n",
    "        tables = soup.find_all('div', class_='Table__ScrollerWrapper')\n",
    "        \n",
    "        for i, table in enumerate(tables):\n",
    "            if i >= len(titles): break\n",
    "            year = titles[i]\n",
    "            \n",
    "            for tr in table.find_all('tr', class_='Table__TR--sm'):\n",
    "                tds = tr.find_all('td')\n",
    "                if len(tds) < 4: continue\n",
    "                \n",
    "                # Check if it's a result (has a score) or a fixture\n",
    "                score_text = tds[2].text.strip()\n",
    "                if \" - \" not in score_text: continue # Skip if no score yet\n",
    "                Hist.append({\n",
    "                    \"Date\": tds[0].text.strip() + \" \" + year, # e.g., \"Sat, 10 Aug 2024\"\n",
    "                    \"Home\": tds[1].text.strip(),\n",
    "                    \"Score\": score_text,\n",
    "                    \"Away\": tds[3].text.strip(),\n",
    "                    \"Competition\": row[\"Competition\"]\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {row['LinkTeam']}: {e}\")\n",
    "\n",
    "# --- 3. CONVERT TO DATAFRAME ---\n",
    "if Hist:\n",
    "    print(f\"‚úÖ Successfully collected {len(Hist)} historical matches.\")\n",
    "else:\n",
    "    print(\"‚ùå No matches found. Check if the ESPN URLs are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Create DataFrame and drop duplicates\n",
    "History = pd.DataFrame(Hist).drop_duplicates()\n",
    "\n",
    "# 2. Safely drop 'Team' ONLY if it exists to prevent the KeyError\n",
    "if 'Team' in History.columns:\n",
    "    History = History.drop(\"Team\", axis=1)\n",
    "\n",
    "# 3. Clean Score and calculate Total Goals\n",
    "def extract_clean_score(score_str):\n",
    "    # Regex to find \"number - number\" patterns, ignoring extra text\n",
    "    match = re.search(r'(\\d+)\\s*-\\s*(\\d+)', str(score_str))\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2))\n",
    "    return None, None # For games without a valid score\n",
    "\n",
    "# Apply the cleaning function\n",
    "scores = History['Score'].apply(extract_clean_score)\n",
    "History[['ScoreH', 'ScoreA']] = pd.DataFrame(scores.tolist(), index=History.index)\n",
    "\n",
    "# Drop matches where the score couldn't be parsed (e.g. \"Postponed\" or \"Suspended\")\n",
    "History = History.dropna(subset=['ScoreH', 'ScoreA'])\n",
    "\n",
    "# Ensure they are integers and calculate total goals\n",
    "History['ScoreH'] = History['ScoreH'].astype(int)\n",
    "History['ScoreA'] = History['ScoreA'].astype(int)\n",
    "History['But'] = History['ScoreH'] + History['ScoreA']\n",
    "\n",
    "# 4. Result Logic using vectorized np.select (cleaner than nested np.where)\n",
    "conditions = [\n",
    "    (History['ScoreH'] == History['ScoreA']),\n",
    "    (History['ScoreH'] > History['ScoreA'])\n",
    "]\n",
    "choices = ['Draw', 'Home']\n",
    "History['Result'] = np.select(conditions, choices, default='Away')\n",
    "\n",
    "# 5. Binary Outcome Columns (Vectorized is 10x faster than .apply)\n",
    "History['Ds'] = (History['Result'] == 'Draw').astype(int)\n",
    "History['Hs'] = (History['Result'] == 'Home').astype(int)\n",
    "History['As'] = (History['Result'] == 'Away').astype(int)\n",
    "History['DA'] = History['Result'].isin(['Draw', 'Away']).astype(int)\n",
    "History['DH'] = History['Result'].isin(['Draw', 'Home']).astype(int)\n",
    "History['HA'] = History['Result'].isin(['Home', 'Away']).astype(int)\n",
    "\n",
    "# 6. Both Teams to Score (LDEM) logic\n",
    "History['LDEMs'] = ((History['ScoreH'] > 0) & (History['ScoreA'] > 0)).astype(int)\n",
    "History['LDEMPs'] = (History['LDEMs'] == 0).astype(int)\n",
    "\n",
    "# 7. Combined Markets (Complex Logic)\n",
    "History['DALDEMs'] = (History['LDEMs'] & History['DA']).astype(int)\n",
    "History['DHLDEMs'] = (History['LDEMs'] & History['DH']).astype(int)\n",
    "History['HALDEMs'] = (History['LDEMs'] & History['HA']).astype(int)\n",
    "History['DALDEMPs'] = (History['LDEMPs'] & History['DA']).astype(int)\n",
    "History['DHLDEMPs'] = (History['LDEMPs'] & History['DH']).astype(int)\n",
    "History['HALDEMPs'] = (History['LDEMPs'] & History['HA']).astype(int)\n",
    "\n",
    "# 8. Date Conversion - Handling the ESPN format \"Sat, 10 Aug 2024\"\n",
    "# Using pd.to_datetime is safer than strptime inside an apply\n",
    "History['Date'] = pd.to_datetime(History['Date'], format=\"%a, %d %b %Y\", errors='coerce')\n",
    "\n",
    "# Drop any rows where the date failed to parse (optional)\n",
    "History = History.dropna(subset=['Date'])\n",
    "\n",
    "# 9. Create unique IdGame and sort\n",
    "History[\"IdGame\"] = History[\"Date\"].dt.strftime('%Y-%m-%d') + History[\"Home\"] + History[\"Away\"]\n",
    "History = History.sort_values('Date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully added 2 new historical matches to History.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- Save History ---\n",
    "file_path = \"History.csv\"\n",
    "\n",
    "# 1. Load existing data or create empty DF\n",
    "if os.path.exists(file_path):\n",
    "    csv_df = pd.read_csv(file_path)\n",
    "    # Ensure ID is string for robust comparison\n",
    "    csv_df['IdGame'] = csv_df['IdGame'].astype(str)\n",
    "else:\n",
    "    # If file doesn't exist, create an empty one with History's columns\n",
    "    csv_df = pd.DataFrame(columns=History.columns)\n",
    "\n",
    "# 2. Identify brand new matches (Fast Method)\n",
    "# Using .isin() is significantly faster than the 'bool' function with .contains()\n",
    "new_ids = History['IdGame'].astype(str)\n",
    "existing_ids = csv_df['IdGame'].unique()\n",
    "\n",
    "is_new = ~new_ids.isin(existing_ids)\n",
    "\n",
    "# 3. Concatenate and Cleanup\n",
    "if is_new.any():\n",
    "    new_data = History[is_new]\n",
    "    \n",
    "    # Merge new matches into the existing database\n",
    "    updated_csv = pd.concat([csv_df, new_data], ignore_index=True)\n",
    "    \n",
    "    # Standardize dates\n",
    "    updated_csv['Date'] = pd.to_datetime(updated_csv['Date'])\n",
    "    \n",
    "    # Remove any accidental duplicates and sort by newest first\n",
    "    updated_csv = updated_csv.drop_duplicates(subset=['IdGame'])\n",
    "    updated_csv = updated_csv.sort_values(by='Date', ascending=False)\n",
    "    \n",
    "    # Save to file\n",
    "    updated_csv.to_csv(file_path, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ Successfully added {len(new_data)} new historical matches to {file_path}.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No new matches to add. History.csv is already up to date.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
